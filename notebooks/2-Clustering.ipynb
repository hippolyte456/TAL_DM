{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des techniques de clustering\n",
    "\n",
    "Le but de ce tp est de faire face à la problématique: \n",
    "<center style=\"color:red\" >  Voici XXX documents -bruts, non étiquetés-... Comment les valoriser? Les exploiter? Les comprendre? Les résumer? </center>\n",
    "\n",
    "Nous avons vu dans les séances précédentes comment représenter les données textuelles sous forme de sacs de mots:\n",
    "$$X = \n",
    "\t\\begin{matrix} \n",
    "\t & \\textbf{t}_j \\\\\n",
    "\t & \\downarrow \\\\\n",
    "\t\\textbf{d}_i \\rightarrow &\n",
    "\t\\begin{pmatrix} \n",
    "\tx_{1,1} & \\dots & x_{1,D} \\\\\n",
    "\t\\vdots & \\ddots & \\vdots \\\\\n",
    "\tx_{N,1} & \\dots & x_{N,D} \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\end{matrix}\n",
    "\t$$\n",
    "\n",
    "A partir de cette représentation, les questions qui se posent sont les suivantes:\n",
    "1. Quel algorithme de clustiering choisir?\n",
    "    - K-means, LSA, pLSA, LDA\n",
    "1. Quels résultats attendre?\n",
    "    - qualité, bruit, exploitabilité immédiate etc...\n",
    "1. Quelles analyses qualitatives effectuer pour comprendre les groupes?\n",
    "1. Comment boucler, itérer pour améliorer la qualité du processus?\n",
    "\n",
    "\n",
    "> <span style=\"color:magenta\" > La tâche est difficile, dans ce TP, on part d'un **jeu de données étiquetées** afin de pouvoir appliquer des métriques quantitatives sur les résultats obtenus. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n",
      "157.9958458546933\n",
      "1787565\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# conversion BoW + tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() # TfidfVectorizer(max_features=500)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(vectors.shape)\n",
    "\n",
    "# mesure de la sparsité = 157 mots actif par document sur 130000 !!\n",
    "print(vectors.nnz / float(vectors.shape[0]))\n",
    "print(vectors.nnz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "print( vectors[0].getnnz() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippo/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(53005, 'evolves'), (30004, 'astrophysicists'), (21076, '8jdf'), (37706, 'capacities'), (22791, '9lby_2y'), (94626, 'pne1t'), (114545, 'theopompus'), (53223, 'excontexts'), (104041, 's5w_p7'), (113006, 't7113p')]\n"
     ]
    }
   ],
   "source": [
    "# retrouver les mots\n",
    "print([(i,vectorizer.get_feature_names()[i]) \\\n",
    "       for i in np.random.randint(vectors.shape[1], size=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  4  4  1 14 16 13  3  2  4]\n",
      "['rec.autos', 'comp.sys.mac.hardware', 'comp.sys.mac.hardware', 'comp.graphics', 'sci.space', 'talk.politics.guns', 'sci.med', 'comp.sys.ibm.pc.hardware', 'comp.os.ms-windows.misc', 'comp.sys.mac.hardware']\n"
     ]
    }
   ],
   "source": [
    "# gestion des étiquettes (pour l'évaluation seulemnet en non-supervisé)\n",
    "Y = newsgroups_train.target\n",
    "print(Y[:10]) # numérique\n",
    "print([newsgroups_train.target_names[i] for i in Y[:10]]) # vraie classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests préliminaires\n",
    "\n",
    "Commençons par le commencement: tout problème non-supervisé (ou presque) doit être analysé en premier lieu avec les $k$-means!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Algo => risque de prendre du temps si le vocabulaire n'a pas été réduit !!\n",
    "# Note: on dirait que l'algo transforme les données en dense vector=> catastrophe pour nous !!!\n",
    "# => limitation du nombre d'itération arbitraire + limitation du vocabulaire\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=0, max_iter=10).fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30827, 114455, 50527, 58239, 114455, 114455, 114455, 105492, 67773, 40658, 89513, 117994, 58487, 114455, 114455, 114455, 114455, 114455, 86702, 86099]\n",
      "ax\n",
      "the\n",
      "edu\n",
      "gatech\n",
      "the\n",
      "the\n",
      "the\n",
      "scsi\n",
      "intercon\n",
      "cmu\n",
      "ohio\n",
      "uga\n",
      "geb\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "netcom\n",
      "nasa\n"
     ]
    }
   ],
   "source": [
    "# analyse\n",
    "\n",
    "# recupération des proto:\n",
    "centres = kmeans.cluster_centers_\n",
    "\n",
    "# mots les plus importants par cluster => TODO\n",
    "ind_mots_freq = []\n",
    "for i in range( centres.shape[0] ):\n",
    "    ind_mots_freq.append(centres[i].argmax())\n",
    "print(ind_mots_freq)\n",
    "\n",
    "for num_words in ind_mots_freq:\n",
    "    print(vectorizer.get_feature_names()[num_words])\n",
    "\n",
    "# version print / version word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limites\n",
    "\n",
    "- Limites liées à la description\n",
    "    - trop de mots\n",
    "    - trop de mots fréquents qui déroutent l'algorithme\n",
    "    - ...\n",
    "- Limites liées à l'algorithme\n",
    "    - distance euclidienne absurde\n",
    "\n",
    "Les limites algorithmiques vont être résolues en changeant d'algorithme... Les limites de représentation des données seront résolues par votre capacité en ingénierie.\n",
    "\n",
    "\n",
    "Algorithmes à tester:\n",
    "- LSA\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD\n",
    "- LDA\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "**Note:** pour des tests rapides, il est plus simple de rester dans le cadre de scikit-learn... Néanmoins, dans un milieu industriel, il faudrait exploiter des outils plus efficaces comme ceux présents dans la librairie ```gensim```. Si vous vous sentez à l'aise avec la donnée textuelles, allez directement vers ces outils:\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64896, 49023, 58776, 49397, 104576, 55006, 26879, 89692, 90260, 63314, 51728, 114455, 30131, 48247, 41358, 31070, 93241, 59515, 37383, 28258]\n",
      "hydro\n",
      "drieux\n",
      "gerald\n",
      "duke\n",
      "sandvik\n",
      "feustel\n",
      "ai\n",
      "oldham\n",
      "oracle\n",
      "hiram\n",
      "engin\n",
      "the\n",
      "ati\n",
      "dma\n",
      "compaq\n",
      "b30\n",
      "petch\n",
      "gmi\n",
      "caldwell\n",
      "angmar\n"
     ]
    }
   ],
   "source": [
    "lda = sklearn.decomposition.LatentDirichletAllocation(n_components=20,random_state=0)\n",
    "lda.fit(vectors)\n",
    "# recupération des proto:\n",
    "centres = lda.components_\n",
    "\n",
    "# mots les plus importants par cluster => TODO\n",
    "ind_mots_freq = []\n",
    "for i in range( centres.shape[0] ):\n",
    "    ind_mots_freq.append(centres[i].argmax())\n",
    "print(ind_mots_freq)\n",
    "\n",
    "for num_words in ind_mots_freq:\n",
    "    print(vectorizer.get_feature_names()[num_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = sklearn.decomposition.TruncatedSVD(n_components= 19, n_iter=10, random_state=0)\n",
    "svd.fit(vectors)\n",
    "\n",
    "\n",
    "# recupération des proto:\n",
    "centres = svd.components_\n",
    "\n",
    "# mots les plus importants par cluster => TODO\n",
    "ind_mots_freq = []\n",
    "for i in range( centres.shape[0] ):\n",
    "    ind_mots_freq.append(centres[i].argmax())\n",
    "print(ind_mots_freq)\n",
    "\n",
    "for num_words in ind_mots_freq:\n",
    "    print(vectorizer.get_feature_names()[num_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des performances\n",
    "\n",
    "Les performances sont très dures à évaluer en clustering... Ce qui explique que cette évaluation est souvent, au moins partiellement, qualitative (=étudiée à l'oeil, sur des exemples ou des paramètres). \n",
    "Afin d'éviter de faire n'importe quoi, il faut aussi réfléchir à des métriques quantitatives.\n",
    "\n",
    "### Qualitatif\n",
    "\n",
    "Analyser le vocabulaire des différents clusters\n",
    "1. En terme de mots les plus fréquents, les plus probables ou de dimensions des vecteurs propres les plus fortes.\n",
    "1. En terme de mots discriminants\n",
    "    - construction de critère de contraste (type odd's ratio) entre la présence dans une classe et présence dans les autres classe\n",
    "1. Affichage \n",
    "    - des 10 ou 20 mots critiques de chaque classe ```print```\n",
    "    - sous la forme de word cloud\n",
    "    - affichage interactif avancé: http://www.kennyshirley.com/LDAvis/\n",
    "        - pour une version intégrable dans un notebook: https://github.com/bmabey/pyLDAvis\n",
    "        - merci de l'utiliser **après avoir compris le principe de réduction de la dimensionalité pour les clusters**\n",
    "    \n",
    "### Quantitatif\n",
    "\n",
    "Pour donner des chiffres, il faut des étiquettes. C'est rarement le cas sur des jeux de données industriels... Mais c'est bon dans un cadre académique comme 20 newsgroups!\n",
    "\n",
    "**Problème:** Comme nos algorithmes sont non supervisés, les sorties (bien que catégorielles) ne sont pas alignées avec l'encodage des étiquettes du jeu de données. Il faut trouver des astuces.\n",
    "\n",
    "1. Etude basique sur la taille des clusters\n",
    "    - est ce qu'une classe n'a pas tout attrapé?\n",
    "1. Pureté = extraction de la classe majoritaire dans un cluster + calcul de la pureté du cluster\n",
    "    - 1 score par cluster par défaut\n",
    "    - agrégation par somme pondérée sur la taille des clusters\n",
    "1. Indice de Rand  https://fr.wikipedia.org/wiki/Indice_de_Rand\n",
    "1. Métrique adaptée à une hypothèse spécifique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vers une version plus évoluée des algorithmes\n",
    "\n",
    "1. Si l'un des clusters attiré toutes les données: êtes-vous capable de supprimer ce cluster et de simplement répartir les échantillons dans les autres catégories?\n",
    "\n",
    "1. Si vous avez une idée vague des thématiques que vous souhaitez voir isolées... \n",
    "    - trouver 10 mots dans chaque catégories\n",
    "    - biaiser l'initialisation pour attirer ces classes\n",
    "\n",
    "1. Si vous mettez un utilisateur dans la boucle\n",
    "    - passer en mode supervisé multiclasse et exploiter les feedbacks de l'utilisateur pour forcer le passage d'un document dans la classe d'à coté \n",
    "        - Naive Bayes, SVM ou autre...\n",
    "    - réfléchir à une approche active qui sélectionne les documents les plus intéressants à montrer à l'utilisateur pour lui demander un étiquetage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "baacd7ded0742aa8408bda3ed6ced71320ba4869ece4e05e453d0cf31ed1376f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
